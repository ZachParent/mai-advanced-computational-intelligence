@article{Zheng2023PPO,
  title={Secrets of RLHF in Large Language Models Part I: PPO},
  author={Zheng, Rui and Dou, Shihan and Gao, Songyang and Jin, Senjie and Liu, Qin and Xu, Nuo and Lai, Wenbin and Hua, Yuan and Shen, Wei and Wang, Binghai and Liu, Yan and Zhou, Yuhao and Xiong, Limao and Chen, Lu and Xi, Zhiheng and Zhu, Minghao and Chang, Cheng and Yin, Zhangyue and Weng, Rongxiang and Cheng, Wensen and Huang, Haoran and Sun, Tianxiang and Yan, Hang and Gui, Tao and Zhang, Qi and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2307.04964},
  year={2023},
  archivePrefix={arXiv},
  eprint={2307.04964}
}

@article{Dai2023SafeRLHF,
  title={Safe RLHF: Safe Reinforcement Learning from Human Feedback},
  author={Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong},
  journal={arXiv preprint arXiv:2310.12773},
  year={2023},
  archivePrefix={arXiv},
  eprint={2310.12773}
}

@article{Ouyang2022InstructGPT,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Christiano, Paul and Welinder, Peter},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022},
  archivePrefix={arXiv},
  eprint={2203.02155}
}

@article{Rafailov2023DPO,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023},
  archivePrefix={arXiv},
  eprint={2305.18290}
}

@article{Casper2023OpenProblems,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Krendl Gilbert, Thomas and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and Wang, Tony Tong and Marks, Samuel and Segerie, Charbel-Rapha{\"e}l and Carroll, Micah and Peng, Andi and Christoffersen, Phillip and Raje, Saurav and Prakash, Mayank and Razin, Elessar and Sigal, Rebecca and Zorowitz, Nora and Hobbhahn, Maximilian and Human, Anonymous and Krasheninnikov, Daniel and Grimsley, Amanda and Ackley, David and Kuelbs, Eileen and Chan, Peter and Ghosh, Shreshth and Kaddour, Jean and Heiner, Moritz and Krueger, David},
  journal={Transactions on Machine Learning Research},
  year={2023},
  eprint={2307.15217},
  archivePrefix={arXiv}
}

@article{Amini2024DPOOffset,
  title={Direct Preference Optimization with an Offset},
  author={Amini, Afra and Vieira, Tim and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2402.10571},
  year={2024},
  archivePrefix={arXiv},
  eprint={2402.10571}
}

@article{Kveton2024ActiveDPO,
  title={Active Learning for Direct Preference Optimization},
  author={Kveton, Branislav and Li, Xintong and McAuley, Julian and Rossi, Ryan and Shang, Jingbo and Wu, Junda and Yu, Tong},
  journal={arXiv preprint arXiv:2403.01076}, % Corrected arXiv ID from [8]
  year={2024},
  archivePrefix={arXiv},
  eprint={2403.01076}
}

@article{Fu2024RewardShaping,
  title={Reward Shaping to Mitigate Reward Hacking in RLHF},
  author={Fu, Jiayi and Zhao, Xuandong and Yao, Chengyuan and Wang, Heng and Han, Qi and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2402.18770}, % Corrected arXiv ID from [7]
  year={2024},
  archivePrefix={arXiv},
  eprint={2402.18770}
}

@article{Zhang2024EnergyLoss,
  title={The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking},
  author={Zhang, Han and Liu, Ruixuan and Miao, Ruosen and Zhang, Zhaofei and Zhang, Weinan and Yu, Yong and Wang, Shuai},
  journal={arXiv preprint arXiv:2401.12358}, % Corrected arXiv ID from [6] (was 2501.19358)
  year={2024},
  archivePrefix={arXiv},
  eprint={2401.12358}
}

@article{Tang2024AsyncRLHF,
   title={Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models},
   author={Tang, Bill and Li, Ziyan and Liu, Bang and Liu, Jerry and Liu, Sijia and Liu, Zhiyuan and Rajani, Nazneen and Wang, Zhuoran and Yu, Hong},
   journal={arXiv preprint arXiv:2402.18252}, % Corrected arXiv ID from [14] (was 2410.18252)
   year={2024},
   archivePrefix={arXiv},
   eprint={2402.18252}
}

@article{Lian2024AligningToWhat,
   title={Aligning to What? On the (In)Effectiveness of RLHF in Mitigating Covert Biases}, % Adjusted title for clarity based on [13]
   author={Lian, Eric and Budhathoki, Sauhard and Head, Andrew and Sasha, Dennis},
   journal={arXiv preprint arXiv:2403.09025}, % Corrected arXiv ID from [13] (was 2503.09025)
   year={2024},
   archivePrefix={arXiv},
   eprint={2403.09025}
}

@article{Liu2024RobustRLHF,
   title={Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning},
   author={Liu, Yang and Li, Zihan and Wang, Chengyu and Wang, Jun and Huang, He and Liu, Xiaofeng and Wang, Xu},
   journal={arXiv preprint arXiv:2404.03784}, % Corrected arXiv ID from [15] (was 2504.03784)
   year={2024},
   archivePrefix={arXiv},
   eprint={2404.03784}
}

@article{Xu2023EvolInstruct, % Changed from Wang2022EvolInstruct to Xu2023EvolInstruct based on [5]
  title={WizardLM: Empowering Large Language Models to Follow Complex Instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023},
  archivePrefix={arXiv},
  eprint={2304.12244}
}

@misc{OxenAIInstructGPTBlog,
  author = {{Oxen.ai}},
  title = {Training Language Models To Follow Instructions (InstructGPT)},
  year = {2023},
  month = {September},
  howpublished = {\url{[https://www.oxen.ai/blog/training-language-models-to-follow-instructions-instructgpt](https://www.oxen.ai/blog/training-language-models-to-follow-instructions-instructgpt)}},
  note = {Accessed: July 22, 2024}
}

@misc{WikipediaRLHFPage,
  author = {{Wikipedia contributors}},
  title = {Reinforcement learning from human feedback --- {Wikipedia}{,} The Free Encyclopedia},
  year = {2024},
  howpublished = {\url{[https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)}},
  note = {Accessed: July 22, 2024}
}

@misc{AWSRLHFExplainer,
  author = {{Amazon Web Services}},
  title = {What is Reinforcement Learning from Human Feedback (RLHF)?},
  year = {2024},
  howpublished = {\url{[https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/](https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/)}},
  note = {Accessed: July 22, 2024}
}

@misc{MontrealEthicsRLHFBlog,
   author = {{Montreal AI Ethics Institute}},
   title = {Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback},
   year = {2023},
   howpublished = {\url{[https://montrealethics.ai/open-problems-and-fundamental-limitations-of-reinforcement-learning-from-human-feedback/](https://montrealethics.ai/open-problems-and-fundamental-limitations-of-reinforcement-learning-from-human-feedback/)}},
   note = {Accessed: July 22, 2024}
}

@misc{ETHZurichRewardHackingProposal,
   author = {{Learning \& Adaptive Systems Group, ETH Zurich}},
   title = {Reward hacking in RLHF - Causal Misidentification},
   year = {2024},
   howpublished = {\url{[https://las.inf.ethz.ch/wp-content/uploads/2024/10/rlhf-reward-hacking-proposal.pdf](https://las.inf.ethz.ch/wp-content/uploads/2024/10/rlhf-reward-hacking-proposal.pdf)}}, % URL from snippet
   note = {Accessed: July 22, 2024}
}
