\documentclass{article}

\usepackage{graphicx} % For including images
\usepackage{amsmath}  % For advanced math typesetting
\usepackage{geometry} % For setting page margins
\geometry{a4paper, margin=1in} % Example margin settings
\usepackage[utf8]{inputenc} % Input encoding
\usepackage[T1]{fontenc}    % Font encoding
\usepackage{hyperref} % For hyperlinks (optional, but good practice)
\usepackage{listings}
\usepackage{subcaption}
\usepackage{caption} % Add the caption package

\hypersetup{
    pdftitle={My Report Title},
    pdfpagemode=FullScreen,
    }

\lstset{
    language=Python,
    breaklines=true,
    basicstyle=\ttfamily,
    commentstyle={},
    frame=single,
    showstringspaces=false,
}

\title{Optimizing PPO Hyperparameters \\ for Continuous Control Tasks}
\author{Zachary Parent \\ Universitat PolitÃ¨cnica de Catalunya}
\date{\today}

\begin{document}

\begin{titlepage} % Start titlepage environment
    \maketitle
    % \thispagestyle{empty} % Not needed, titlepage environment handles this

    \begin{abstract}
    Proximal Policy Optimization (PPO) is a widely-used reinforcement learning algorithm, but its performance often depends on hyperparameter tuning. This report investigates the sensitivity of PPO to actor and critic learning rates across three continuous control tasks (Pendulum-v1, InvertedPendulum-v5, Ant-v5) using the Gymnasium library. Following established implementation details, we performed a grid search over learning rates, executing multiple runs per configuration for statistical robustness via t-tests. Results indicate that within the tested range (\(3 \times 10^{-3}\) to \(3 \times 10^{-4}\)), the critic learning rate had no statistically significant effect on final performance across the environments (\(p > 0.05\)). In contrast, the actor learning rate significantly impacted results, with the optimal rate varying by environment: the baseline \(3 \times 10^{-4}\) was best for Pendulum and Ant, while a higher rate (\(1 \times 10^{-3}\)) excelled in the simpler InvertedPendulum task. These findings highlight the critical role of the actor learning rate and its potential environment-dependency in achieving optimal PPO performance.
    \end{abstract}
    \tableofcontents
    \thispagestyle{empty}

\end{titlepage} % End titlepage environment


% \clearpage % Not needed, titlepage ends the page
% \setcounter{page}{1} % Usually not needed, numbering starts correctly after titlepage

\section{Introduction}
Reinforcement Learning (RL) offers powerful techniques for training agents to perform complex tasks through interaction with an environment. Among the various RL algorithms, Proximal Policy Optimization (PPO) stands out as a popular and effective choice due to its balance of sample efficiency, implementation simplicity, and robust performance across diverse domains \cite{schulman2017proximalpolicyoptimizationalgorithms}. However, like many deep RL algorithms, PPO's performance can be sensitive to the choice of hyperparameters.

This report focuses on investigating the impact of two critical hyperparameters: the learning rates for the actor (policy) and critic (value function) networks within a PPO agent. Specifically, we aim to understand how different combinations of these learning rates affect the final performance of the PPO algorithm when applied to a set of standard continuous control benchmark tasks. We utilized the Gymnasium library \cite{towers2024gymnasium} to implement and test the agent on the Pendulum-v1, InvertedPendulum-v5, and Ant-v5 environments, representing varying levels of task complexity. The subsequent sections detail our methodology, present the experimental results including statistical analyses, and conclude with a discussion of the findings, limitations, and potential future work.

\section{Methodology}

\subsection{Algorithm}
The Proximal Policy Optimization (PPO) algorithm was selected due to its balance between sample efficiency and stability. It is a widely-used, state-of-the-art algorithm suitable for various reinforcement learning tasks. Our implementation closely follows the details outlined in the work by Huang et al. \cite{shengyi2022the37implementation}.

\subsubsection{General PPO Algorithm Details}
The following general PPO implementation details were adopted:
\begin{itemize}
    \item \textbf{Weight Initialization:} Orthogonal initialization for weights and constant initialization for biases were used.
    \item \textbf{Adam Optimizer Epsilon:} The epsilon parameter of the Adam optimizer was set to \(1 \times 10^{-5}\) (Code reference: \texttt{ppo2/model.py\#L100}).
    \item \textbf{Adam Learning Rate Annealing:} The learning rate for the Adam optimizer was linearly decayed from \(3 \times 10^{-4}\) to 0 over the course of training, following common practice for MuJoCo environments (Code reference: \texttt{ppo2/ppo2.py\#L133-L135}).
    \item \textbf{Generalized Advantage Estimation (GAE):} GAE was employed for estimating advantage values.
    \item \textbf{Advantage Normalization:} Calculated advantages were normalized at the minibatch level by subtracting the mean and dividing by the standard deviation.
    \item \textbf{Global Gradient Clipping:} The gradients of the policy and value networks were rescaled during each update iteration to ensure the global L2 norm did not exceed 0.5. This technique has been shown to offer potential performance benefits \cite{andrychowicz2021what}.
    \item \textbf{Separate Networks:} Distinct neural networks were used for the policy (actor) and value (critic) functions. The architectures are outlined below:

    \begin{minipage}{0.9\textwidth}
    \begin{lstlisting}[language=Python]
value_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, 1), std=1.0),
)
policy_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, envs.single_action_space.n), std=0.01),
)
# Usage example:
# value = value_network(observation)
# action_dist = Categorical(policy_network(observation))
# action = action_dist.sample()
    \end{lstlisting}
    \end{minipage}
\end{itemize}

\subsubsection{MuJoCo-Specific PPO Algorithm Details}
For environments involving continuous action spaces, specifically MuJoCo tasks, the following details were incorporated:
\begin{itemize}
    \item \textbf{Continuous Actions:} Actions were sampled from a Normal distribution. The policy network outputs the mean, while the standard deviation is handled separately.
    \item \textbf{State-Independent Log Standard Deviation:} The logarithm of the standard deviation for the action distribution was maintained as a state-independent parameter, initialized to zero.
    \item \textbf{Action Clipping and Storage:} Sampled continuous actions were clipped to the valid range defined by the environment. However, the original, unclipped action was stored in the experience buffer.
    \item \textbf{Reward Scaling:} Rewards were scaled by dividing them by the standard deviation of a rolling discounted sum of rewards. This technique, often implemented via wrappers like \texttt{VecNormalize}, is recommended for potentially improving performance \cite{Engstrom2020Implementation}.
\end{itemize}

\subsection{Environments}
We evaluated the PPO algorithm on three environments from the Gymnasium library \cite{towers2024gymnasium}, selected for their continuous action spaces: Pendulum-v1, InvertedPendulum-v5, and Ant-v5. The Pendulum and Inverted Pendulum environments represent classic control problems with relatively simple dynamics, while the Ant environment presents a more complex locomotion task. This selection allows for testing the algorithm's performance across varying levels of complexity within continuous control scenarios.

\subsection{Experiment Design}

\paragraph{Setup}
To structure the experiments, an abstract \texttt{Agent} class was defined, along with a specific implementation for the \texttt{PPOAgent}. The Gymnasium library facilitated environment creation and interaction. A \texttt{RunConfig} class was implemented to manage experiment configurations, ensuring parameters were consistently applied. Reproducibility was addressed by seeding the environment, agent initialization, and training process for each run.

\paragraph{Parameter Sweep and Replication}
To assess the impact of learning rates on performance, we conducted a grid search over different configurations for the actor and critic optimizers. The searched values were centered around those suggested by Huang et al. \cite{shengyi2022the37implementation}:
\begin{itemize}
    \item Actor Learning Rates: \(1 \times 10^{-3}\), \textbf{\(3 \times 10^{-4}\)} (baseline), \(1 \times 10^{-4}\)
    \item Critic Learning Rates: \(3 \times 10^{-3}\), \textbf{\(1 \times 10^{-3}\)} (baseline), \(3 \times 10^{-4}\)
\end{itemize}
This resulted in 9 distinct learning rate configurations per environment. For statistical robustness, each configuration was executed with 5 different random seeds.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/individual_rewards_curves_pendulum.png}
        \caption{Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/individual_rewards_curves_inverted_pendulum.png}
        \caption{Inverted Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/individual_rewards_curves_ant.png}
        \caption{Ant}
    \end{subfigure}
    \captionsetup{justification=centering} \
    \caption{Rewards curves for each environment. Each row represents a different seed, each column represents a different learning rate configuration}
    \label{fig:rewards_curves}
\end{figure}

\paragraph{Performance Evaluation}
Agent performance was measured by averaging the episodic reward over the final 20 episodes of training. This metric provides a stable estimate of the converged performance for each run. To determine if observed performance differences between learning rate configurations were statistically significant within each environment, independent two-sample t-tests were planned, and an alpha level of 0.05 was fixed a priori.

\paragraph{Total Runs}
The experimental setup resulted in a total of 45 runs per environment (3 actor rates \(\times\) 3 critic rates \(\times\) 5 seeds). Across the three environments, this amounted to 135 individual training runs.
\[
(3 \text{ actor learning rates} \times 3 \text{ critic learning rates} \times 5 \text{ seeds}) \times 3 \text{ environments} = 135 \text{ total runs}
\]

\clearpage
\section{Results}

The configurations for each environment were run long enough to converge. The reward curves, including the actual episode reward and the moving average reward, are shown in  \autoref{fig:rewards_curves}. The reward from the final 20 episodes of each run were averaged to and used as the measure of performance for that run. Further analysis of this performance follows.

\subsection{Rejecting the Null Hypothesis}
\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_pval_pendulum.png}
        \caption{Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_pval_inverted_pendulum.png}
        \caption{Inverted Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_pval_ant.png}
        \caption{Ant}
    \end{subfigure}
    \captionsetup{justification=centering} \
    \caption{Pairwise p-values for each environment. Pairs with a p-value less than $\alpha = 0.05$ are highlighted in yellow.}
    \label{fig:pairwise_comparison_pval}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
            \includegraphics[width=\textwidth]{figures/pairwise_comparison_diff_pendulum.png}
        \caption{Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_diff_inverted_pendulum.png}
        \caption{Inverted Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_diff_ant.png}
        \caption{Ant}
    \end{subfigure}
    \captionsetup{justification=centering} \
    \caption{Pairwise differences for each environment. Pairs with a p-value less than $\alpha = 0.05$ are highlighted in yellow.}
    \label{fig:pairwise_comparison_diff}
\end{figure}


We first aim to reject the null hypothesis, that the mean reward of any two configurations are the same. We do this by performing a two-sample t-test for each environment, for each configuration, for each environment. Recall that we used 5 seeds for each configuration and an alpha level of 0.05. The results are shown in the \autoref{fig:pairwise_comparison_pval}. We immediately notice that certain configurations have a performance advantage which is statistically significant over others. To quantify the effect size of the performance difference, \autoref{fig:pairwise_comparison_diff} shows the difference in performance between the configurations.

\subsection{Identifying the best configurations}

\paragraph{Pendulum}
For the pendulum environment, we see that there is no signifcant difference caused by varying the critic learning rate (\autoref{fig:critic_lr_pval_pendulum}). However, there is a significant difference caused by varying the actor learning rate (\autoref{fig:actor_lr_pval_pendulum}). \autoref{fig:learning_rate_comparison_pendulum} shows the boxen plot of the performance of the different learning rate configurations. The best actor learning rate is \(3E-4\), which is the same as the baseline.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/learning_rate_comparison_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Boxen plot, comparing performance of learning rate configurations}
        \label{fig:learning_rate_comparison_pendulum}
    \end{subfigure}
    \\
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_pval_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Critic LR p-values}
        \label{fig:critic_lr_pval_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_diff_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Critic LR difference}
        \label{fig:critic_lr_diff_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_pval_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Actor LR p-values}
        \label{fig:actor_lr_pval_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_diff_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Actor LR difference}
        \label{fig:actor_lr_diff_pendulum}
    \end{subfigure}
    \caption{Pendulum learning rate comparison}
    \label{fig:pendulum_analysis}
\end{figure}

\clearpage
\paragraph{Inverted Pendulum}
For the Inverted Pendulum environment, we again see that there is no significant difference caused by varying the critic learning rate (\autoref{fig:critic_lr_pval_inverted_pendulum}), although we see some evidence that the \(1E-3\) critic learning rate is superior.

There is a significant difference caused by varying the actor learning rate (\autoref{fig:actor_lr_pval_inverted_pendulum}). \autoref{fig:learning_rate_comparison_inverted_pendulum} shows the boxen plot of the performance of the different learning rate configurations. The best actor learning rate is \(1E-3\), which differs from the baseline of \(3E-4\). We expect this higher learning rate to be better because the environment is so simple, and there is an advantage to be gained from a more aggressive learning rate. The effect size of the difference (\autoref{fig:actor_lr_diff_inverted_pendulum}) is not so large, but it is still significant.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/learning_rate_comparison_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Boxen plot, comparing performance of learning rate configurations}
        \label{fig:learning_rate_comparison_inverted_pendulum}
    \end{subfigure}
    \\
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_pval_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Critic LR p-values}
        \label{fig:critic_lr_pval_inverted_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_diff_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Critic LR difference}
        \label{fig:critic_lr_diff_inverted_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_pval_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Actor LR p-values}
        \label{fig:actor_lr_pval_inverted_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_diff_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Actor LR difference}
        \label{fig:actor_lr_diff_inverted_pendulum}
    \end{subfigure}
    \caption{Inverted Pendulum learning rate comparison}
    \label{fig:inverted_pendulum_analysis}
\end{figure}
\clearpage

\paragraph{Ant}
For the Ant environment, we again see that there is no significant difference caused by varying the critic learning rate (\autoref{fig:critic_lr_pval_ant}). However, there is a significant difference caused by varying the actor learning rate (\autoref{fig:actor_lr_pval_ant}). The effect size of the difference (\autoref{fig:actor_lr_diff_ant}) is much larger than that of the other two environments, indicating the importance of tuning the actor learning rate for this complex environment.

\autoref{fig:learning_rate_comparison_ant} shows the boxen plot of the performance of the different learning rate configurations. The best actor learning rate is \(3E-4\), which is the same as the baseline.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/learning_rate_comparison_ant.png}
        \captionsetup{justification=centering}
        \caption{Boxen plot, comparing performance of learning rate configurations}
        \label{fig:learning_rate_comparison_ant}
    \end{subfigure}
    \\
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_pval_ant.png}
        \captionsetup{justification=centering}
        \caption{Critic LR p-values}
        \label{fig:critic_lr_pval_ant}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_diff_ant.png}
        \captionsetup{justification=centering}
        \caption{Critic LR difference}
        \label{fig:critic_lr_diff_ant}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_pval_ant.png}
        \captionsetup{justification=centering}
        \caption{Actor LR p-values}
        \label{fig:actor_lr_pval_ant}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_diff_ant.png}
        \captionsetup{justification=centering}
        \caption{Actor LR difference}
        \label{fig:actor_lr_diff_ant}
    \end{subfigure}
    \caption{Ant learning rate comparison}
    \label{fig:ant_analysis}
\end{figure}

\clearpage
\section{Conclusion}

\paragraph{Summary of Work}
This report detailed the implementation of the Proximal Policy Optimization (PPO) algorithm, closely adhering to the best practices outlined by Huang et al. \cite{shengyi2022the37implementation}. We conducted a systematic investigation into the effects of actor and critic learning rates on agent performance across three continuous control environments from the Gymnasium library: Pendulum-v1, InvertedPendulum-v5, and Ant-v5.

\paragraph{Rigorous Experimental Design}
The experimental methodology prioritized statistical robustness and reproducibility. By employing multiple random seeds (5 per configuration), performing a grid search over 9 learning rate combinations per environment, and utilizing independent t-tests with a pre-defined alpha level (\(\alpha=0.05\)), we aimed to draw reliable conclusions about the impact of these specific hyperparameters. This careful setup allowed for the identification of statistically significant performance differences.

\paragraph{Key Findings on Learning Rates}
Our analysis revealed distinct patterns regarding learning rate sensitivity. Across all three tested environments, variations in the \textit{critic} learning rate within the tested range (\(3 \times 10^{-3}\) to \(3 \times 10^{-4}\)) did not yield statistically significant differences in final performance. Conversely, the \textit{actor} learning rate proved to be a critical hyperparameter. While the baseline rate (\(3 \times 10^{-4}\)) was optimal for Pendulum and the more complex Ant environment, a higher rate (\(1 \times 10^{-3}\)) performed best on the simpler Inverted Pendulum task. This suggests that while the critic's learning rate might be relatively stable, the optimal actor learning rate can be environment-dependent, potentially correlating with task complexity, and significantly impacts final agent performance, especially in challenging domains like Ant.

\paragraph{Limitations}
This study has several limitations. The investigation was confined to the PPO algorithm and only three environments with continuous action spaces. The grid search explored a limited range of learning rates, and other crucial hyperparameters (e.g., batch size, number of epochs per update, GAE lambda, clipping parameter \(\epsilon\)) were held constant based on common practices. Furthermore, the neural network architecture for the actor and critic was fixed. Interactions between learning rates and these other hyperparameters were not explored.

\paragraph{Future Directions}
Future work could expand upon these findings by: exploring a wider range of learning rates or employing adaptive learning rate schedules; testing sensitivity to other PPO hyperparameters; investigating the influence of network architecture choices; and comparing PPO's sensitivity to that of other reinforcement learning algorithms under similar experimental conditions.

\bibliographystyle{plain} % Style for the bibliography (e.g., plain, unsrt, alpha)
\bibliography{references} % Name of your .bib file (without the .bib extension)

\end{document}
