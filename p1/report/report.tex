\documentclass{article}

\usepackage{graphicx} % For including images
\usepackage{amsmath}  % For advanced math typesetting
\usepackage{geometry} % For setting page margins
\geometry{a4paper, margin=1in} % Example margin settings
\usepackage[utf8]{inputenc} % Input encoding
\usepackage[T1]{fontenc}    % Font encoding
\usepackage{hyperref} % For hyperlinks (optional, but good practice)
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={My Report Title},
    pdfpagemode=FullScreen,
    }

\title{My Research Report Title}
\author{Your Name \\ Your Affiliation}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This is a brief summary of the report's content, highlighting the key findings and conclusions.
\end{abstract}

\section{Introduction}
- background on the task
  - we were asked to implement a reinforcement learning algorithm to solve a task
  - we used the gymnasium library to create the environment
  - we chose PPO as our algorithm
  - we applied the algorithm to multiple environments
  - we were curious about how different configurations of the actor and critic learning rates affected the performance of the algorithm, with respect to each environment

\section{Methodology}

\subsection{Algorithm}
- we selected the PPO algorithm for its balance between sample efficiency and stability, and because it is a popular algorithm for a variety of tasks, and it's state of the art
- we followed implementation details from the 37 implementation details paper \cite{shengyi2022the37implementation}

\subsubsection{General PPO Algorithm Details}
- Orthogonal Initialization of Weights and Constant Initialization of biases
- The Adam Optimizer’s Epsilon Parameter (ppo2/model.py#L100) Code-level Optimizations PPO sets the epsilon parameter to 1e-5
- Adam Learning Rate Annealing (ppo2/ppo2.py#L133-L135) Code-level Optimizations
- The Adam optimizer’s learning rate could be either constant or set to decay. In MuJoCo, the learning rate linearly decays from 3e-4 to 0
- Generalized Advantage Estimation
- Normalization of Advantages (ppo2/model.py#L139) Code-level Optimizations
- After calculating the advantages based on GAE, PPO normalizes the advantages by subtracting their mean and dividing them by their standard deviation. In particular, this normalization happens at the minibatch level instead of the whole batch level!
- Global Gradient Clipping (ppo2/model.py#L102-L108) Code-level Optimizations
- For each update iteration in an epoch, PPO rescales the gradients of the policy and value network so that the “global l2 norm” (i.e., the norm of the concatenated gradients of all parameters) does not exceed 0.5.
Andrychowicz, et al. (2021) \cite{andrychowicz2021what} find global gradient clipping to offer a small performance boost (decision C68, figure 34).
- separate networks for policy and value function
  $$  value_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, 1), std=1.0),
)
policy_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, envs.single_action_space.n), std=0.01),
)
value = value_network(observation)
action = Categorical(policy_network(observation)).sample()$$

\subsubsection{Mujoco PPO Algorithm Details}
- Continuous actions via normal distributions (common/distributions.py#L103-L104) Theory
  - Policy gradient methods (including PPO) assume the continuous actions are sampled from a normal distribution. So to create such distribution, the neural network needs to output the mean and standard deviation of the continuous action.
- State-independent log standard deviation (common/distributions.py#L104) Theory
  - The implementation outputs the logits for the mean, but instead of outputting the logits for the standard deviation, it outputs the logarithm of the standard deviation. In addition, this log std is set to be state-independent and initialized to be 0.
- Handling of action clipping to valid range and storage (common/cmd_util.py#L99-L100) Code-level Optimizations
  - After a continuous action is sampled, such action could be invalid because it could exceed the valid range of continuous actions in the environment. To avoid this, add applies the rapper to clip the action into the valid range. However, the original unclipped action is stored as part of the episodic data
- Reward Scaling (common/vec_env/vec_normalize.py#L28) Environment Preprocessing
  - The VecNormalize also applies a certain discount-based scaling scheme, where the rewards are divided by the standard deviation of a rolling discounted sum of the rewards (without subtracting and re-adding the mean).
  - Engstrom, Ilyas, et al., (2020) \cite{Engstrom2020Implementation} reported that reward scaling can significantly affect the performance of the algorithm and recommends the use of reward scaling.

\subsection{Environments}
- we selected the pendulum, inverted pendulum, and ant environments because they feature continuous action spaces. the pendulum and inverted pendulum are also simple to understand, and the ant environment is a more complex task that is still relatively simple to understand.

\subsection{Experiment Design}
- we created an abstract agent class and a class for the PPO agent
- we used the gymnasium library to create the environment
- we follow

For example, we used the following equation:
\[ E = mc^2 \]
This methodology was chosen because...

\section{Results}
Present the findings of your research. Use figures, tables, and text to clearly show the results.
Refer to Figure \ref{fig:example} for an example.

\begin{figure}[htbp] % h=here, t=top, b=bottom, p=page of floats
    \centering
    % \includegraphics[width=0.8\textwidth]{path/to/your/figure.png} % Uncomment and replace path
    \fbox{Your figure would go here} % Placeholder for the figure
    \caption{An example figure caption.}
    \label{fig:example}
\end{figure}

Discuss the results and their significance. For instance, we observed that... This contradicts the findings of \cite{dummy_key2}.

\section{Conclusion}
Summarize the main findings of the report. Discuss the implications of your results, acknowledge any limitations, and suggest potential future work.

\bibliographystyle{plain} % Style for the bibliography (e.g., plain, unsrt, alpha)
\bibliography{references} % Name of your .bib file (without the .bib extension)

\end{document}
