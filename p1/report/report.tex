\documentclass{article}

\usepackage{graphicx} % For including images
\usepackage{amsmath}  % For advanced math typesetting
\usepackage{geometry} % For setting page margins
\geometry{a4paper, margin=1in} % Example margin settings
\usepackage[utf8]{inputenc} % Input encoding
\usepackage[T1]{fontenc}    % Font encoding
\usepackage{hyperref} % For hyperlinks (optional, but good practice)
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={My Report Title},
    pdfpagemode=FullScreen,
    }

\title{My Research Report Title}
\author{Your Name \\ Your Affiliation}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This is a brief summary of the report's content, highlighting the key findings and conclusions.
\end{abstract}

\section{Introduction}
- background on the task
  - we were asked to implement a reinforcement learning algorithm to solve a task
  - we used the gymnasium library to create the environment
  - we chose PPO as our algorithm
  - we applied the algorithm to multiple environments
  - we were curious about how different configurations of the actor and critic learning rates affected the performance of the algorithm, with respect to each environment

\section{Methodology}

\subsection{Algorithm}
The Proximal Policy Optimization (PPO) algorithm was selected due to its balance between sample efficiency and stability. It is a widely-used, state-of-the-art algorithm suitable for various reinforcement learning tasks. Our implementation closely follows the details outlined in the work by Huang et al. \cite{shengyi2022the37implementation}.

\subsubsection{General PPO Algorithm Details}
The following general PPO implementation details were adopted:
\begin{itemize}
    \item \textbf{Weight Initialization:} Orthogonal initialization for weights and constant initialization for biases were used.
    \item \textbf{Adam Optimizer Epsilon:} The epsilon parameter of the Adam optimizer was set to \(1 \times 10^{-5}\) (Code reference: \texttt{ppo2/model.py\#L100}).
    \item \textbf{Adam Learning Rate Annealing:} The learning rate for the Adam optimizer was linearly decayed from \(3 \times 10^{-4}\) to 0 over the course of training, following common practice for MuJoCo environments (Code reference: \texttt{ppo2/ppo2.py\#L133-L135}).
    \item \textbf{Generalized Advantage Estimation (GAE):} GAE was employed for estimating advantage values.
    \item \textbf{Advantage Normalization:} Calculated advantages were normalized at the minibatch level by subtracting the mean and dividing by the standard deviation (Code reference: \texttt{ppo2/model.py\#L139}).
    \item \textbf{Global Gradient Clipping:} The gradients of the policy and value networks were rescaled during each update iteration to ensure the global L2 norm did not exceed 0.5. This technique has been shown to offer potential performance benefits \cite{andrychowicz2021what}.
    \item \textbf{Separate Networks:} Distinct neural networks were used for the policy (actor) and value (critic) functions. The architectures are outlined below:
    \begin{verbatim}
value_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, 1), std=1.0),
)
policy_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, envs.single_action_space.n), std=0.01),
)
# Usage example:
# value = value_network(observation)
# action_dist = Categorical(policy_network(observation))
# action = action_dist.sample()
    \end{verbatim}
\end{itemize}

\subsubsection{MuJoCo-Specific PPO Algorithm Details}
For environments involving continuous action spaces, specifically MuJoCo tasks, the following details were incorporated:
\begin{itemize}
    \item \textbf{Continuous Actions:} Actions were sampled from a Normal distribution. The policy network outputs the mean, while the standard deviation is handled separately.
    \item \textbf{State-Independent Log Standard Deviation:} The logarithm of the standard deviation for the action distribution was maintained as a state-independent parameter, initialized to zero (Code reference: \texttt{common/distributions.py\#L104}).
    \item \textbf{Action Clipping and Storage:} Sampled continuous actions were clipped to the valid range defined by the environment. However, the original, unclipped action was stored in the experience buffer.
    \item \textbf{Reward Scaling:} Rewards were scaled by dividing them by the standard deviation of a rolling discounted sum of rewards. This technique, often implemented via wrappers like \texttt{VecNormalize}, is recommended for potentially improving performance \cite{Engstrom2020Implementation}.
\end{itemize}

\subsection{Environments}
- we selected the pendulum, inverted pendulum, and ant environments because they feature continuous action spaces. the pendulum and inverted pendulum are also simple to understand, and the ant environment is a more complex task that is still relatively simple to understand.

\subsection{Experiment Design}
- we created an abstract agent class and a class for the PPO agent
- we used the gymnasium library to create the environment
- we follow

For example, we used the following equation:
\[ E = mc^2 \]
This methodology was chosen because...

\section{Results}
Present the findings of your research. Use figures, tables, and text to clearly show the results.
Refer to Figure \ref{fig:example} for an example.

\begin{figure}[htbp] % h=here, t=top, b=bottom, p=page of floats
    \centering
    % \includegraphics[width=0.8\textwidth]{path/to/your/figure.png} % Uncomment and replace path
    \fbox{Your figure would go here} % Placeholder for the figure
    \caption{An example figure caption.}
    \label{fig:example}
\end{figure}

Discuss the results and their significance. For instance, we observed that...

\section{Conclusion}
Summarize the main findings of the report. Discuss the implications of your results, acknowledge any limitations, and suggest potential future work.

\bibliographystyle{plain} % Style for the bibliography (e.g., plain, unsrt, alpha)
\bibliography{references} % Name of your .bib file (without the .bib extension)

\end{document}
