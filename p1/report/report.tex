\documentclass{article}

\usepackage{graphicx} % For including images
\usepackage{amsmath}  % For advanced math typesetting
\usepackage{geometry} % For setting page margins
\geometry{a4paper, margin=1in} % Example margin settings
\usepackage[utf8]{inputenc} % Input encoding
\usepackage[T1]{fontenc}    % Font encoding
\usepackage{hyperref} % For hyperlinks (optional, but good practice)
\usepackage{listings}
\usepackage{subcaption}
\usepackage{caption} % Add the caption package

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={My Report Title},
    pdfpagemode=FullScreen,
    }

\lstset{
    language=Python,
    breaklines=true,
    basicstyle=\ttfamily,
    commentstyle={},
    frame=single,
    showstringspaces=false,
}

\title{My Research Report Title}
\author{Your Name \\ Your Affiliation}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This is a brief summary of the report's content, highlighting the key findings and conclusions.
\end{abstract}

\section{Introduction}
- background on the task
  - we were asked to implement a reinforcement learning algorithm to solve a task
  - we used the gymnasium library to create the environment
  - we chose PPO as our algorithm
  - we applied the algorithm to multiple environments
  - we were curious about how different configurations of the actor and critic learning rates affected the performance of the algorithm, with respect to each environment

\section{Methodology}

\subsection{Algorithm}
The Proximal Policy Optimization (PPO) algorithm was selected due to its balance between sample efficiency and stability. It is a widely-used, state-of-the-art algorithm suitable for various reinforcement learning tasks. Our implementation closely follows the details outlined in the work by Huang et al. \cite{shengyi2022the37implementation}.

\subsubsection{General PPO Algorithm Details}
The following general PPO implementation details were adopted:
\begin{itemize}
    \item \textbf{Weight Initialization:} Orthogonal initialization for weights and constant initialization for biases were used.
    \item \textbf{Adam Optimizer Epsilon:} The epsilon parameter of the Adam optimizer was set to \(1 \times 10^{-5}\) (Code reference: \texttt{ppo2/model.py\#L100}).
    \item \textbf{Adam Learning Rate Annealing:} The learning rate for the Adam optimizer was linearly decayed from \(3 \times 10^{-4}\) to 0 over the course of training, following common practice for MuJoCo environments (Code reference: \texttt{ppo2/ppo2.py\#L133-L135}).
    \item \textbf{Generalized Advantage Estimation (GAE):} GAE was employed for estimating advantage values.
    \item \textbf{Advantage Normalization:} Calculated advantages were normalized at the minibatch level by subtracting the mean and dividing by the standard deviation.
    \item \textbf{Global Gradient Clipping:} The gradients of the policy and value networks were rescaled during each update iteration to ensure the global L2 norm did not exceed 0.5. This technique has been shown to offer potential performance benefits \cite{andrychowicz2021what}.
    \item \textbf{Separate Networks:} Distinct neural networks were used for the policy (actor) and value (critic) functions. The architectures are outlined below:
    \begin{lstlisting}[language=Python]
value_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, 1), std=1.0),
)
policy_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, envs.single_action_space.n), std=0.01),
)
# Usage example:
# value = value_network(observation)
# action_dist = Categorical(policy_network(observation))
# action = action_dist.sample()
    \end{lstlisting}
\end{itemize}

\subsubsection{MuJoCo-Specific PPO Algorithm Details}
For environments involving continuous action spaces, specifically MuJoCo tasks, the following details were incorporated:
\begin{itemize}
    \item \textbf{Continuous Actions:} Actions were sampled from a Normal distribution. The policy network outputs the mean, while the standard deviation is handled separately.
    \item \textbf{State-Independent Log Standard Deviation:} The logarithm of the standard deviation for the action distribution was maintained as a state-independent parameter, initialized to zero.
    \item \textbf{Action Clipping and Storage:} Sampled continuous actions were clipped to the valid range defined by the environment. However, the original, unclipped action was stored in the experience buffer.
    \item \textbf{Reward Scaling:} Rewards were scaled by dividing them by the standard deviation of a rolling discounted sum of rewards. This technique, often implemented via wrappers like \texttt{VecNormalize}, is recommended for potentially improving performance \cite{Engstrom2020Implementation}.
\end{itemize}

\subsection{Environments}
- we selected the pendulum, inverted pendulum, and ant environments because they feature continuous action spaces.
- the pendulum and inverted pendulum are also simple to understand, and the ant environment is a more complex task that is still relatively simple to understand.

\subsection{Experiment Design}
- we created an abstract agent class and a class for the PPO agent
- we used the gymnasium library to create the environment
- we created a class RunConfig to store the configuration of the experiment
- we used a seed for the environment, agent, and training process, to ensure reproducibility
- we generated 5 runs for each configuration, to allow for statistical analysis
- we performed a grid search over 3 critic learning rates and 3 actor learning rates, centered around the values used in the 37 implementation details of PPO \cite{shengyi2022the37implementation} (actor learning rates: 1E-3, \textbf{3E-4}, 1E-4; critic learning rates: 3E-3, \textbf{1E-3}, 3E-4)
- we considered the average episode reward over the last 20 episodes of training, to collect a better estimate of the performance of each agent
- by performing independent T-tests, we could determine if the agent's performance was signifcantly affected by the learning rates, in each environment
- a total of 135 runs were performed, 45 for each environment, 9 configurations per environment
$$5 \text{ seeds} \times 3 \text{ critic learning rates} \times 3 \text{ actor learning rates} = 45 \text{ runs per environment}$$
$$45 \text{ runs per environment} \times 3 \text{ environments} = 135 \text{ runs}$$

\section{Results}
\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/individual_rewards_curves_pendulum.png}
        \caption{Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/individual_rewards_curves_inverted_pendulum.png}
        \caption{Inverted Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/individual_rewards_curves_ant.png}
        \caption{Ant}
    \end{subfigure}
    \captionsetup{justification=centering} \
    \caption{Rewards curves for each environment. Each row represents a different seed, each column represents a different learning rate configuration}
\end{figure}

Discuss the results and their significance. For instance, we observed that...

\section{Conclusion}
Summarize the main findings of the report. Discuss the implications of your results, acknowledge any limitations, and suggest potential future work.

\bibliographystyle{plain} % Style for the bibliography (e.g., plain, unsrt, alpha)
\bibliography{references} % Name of your .bib file (without the .bib extension)

\end{document}
