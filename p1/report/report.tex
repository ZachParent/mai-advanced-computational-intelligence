\documentclass{article}

\usepackage{graphicx} % For including images
\usepackage{amsmath}  % For advanced math typesetting
\usepackage{geometry} % For setting page margins
\geometry{a4paper, margin=1in} % Example margin settings
\usepackage[utf8]{inputenc} % Input encoding
\usepackage[T1]{fontenc}    % Font encoding
\usepackage{hyperref} % For hyperlinks (optional, but good practice)
\usepackage{listings}
\usepackage{subcaption}
\usepackage{caption} % Add the caption package

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={My Report Title},
    pdfpagemode=FullScreen,
    }

\lstset{
    language=Python,
    breaklines=true,
    basicstyle=\ttfamily,
    commentstyle={},
    frame=single,
    showstringspaces=false,
}

\title{My Research Report Title}
\author{Your Name \\ Your Affiliation}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This is a brief summary of the report's content, highlighting the key findings and conclusions.
\end{abstract}

\section{Introduction}
- background on the task
  - we were asked to implement a reinforcement learning algorithm to solve a task
  - we used the gymnasium library to create the environment
  - we chose PPO as our algorithm
  - we applied the algorithm to multiple environments
  - we were curious about how different configurations of the actor and critic learning rates affected the performance of the algorithm, with respect to each environment

\section{Methodology}

\subsection{Algorithm}
The Proximal Policy Optimization (PPO) algorithm was selected due to its balance between sample efficiency and stability. It is a widely-used, state-of-the-art algorithm suitable for various reinforcement learning tasks. Our implementation closely follows the details outlined in the work by Huang et al. \cite{shengyi2022the37implementation}.

\subsubsection{General PPO Algorithm Details}
The following general PPO implementation details were adopted:
\begin{itemize}
    \item \textbf{Weight Initialization:} Orthogonal initialization for weights and constant initialization for biases were used.
    \item \textbf{Adam Optimizer Epsilon:} The epsilon parameter of the Adam optimizer was set to \(1 \times 10^{-5}\) (Code reference: \texttt{ppo2/model.py\#L100}).
    \item \textbf{Adam Learning Rate Annealing:} The learning rate for the Adam optimizer was linearly decayed from \(3 \times 10^{-4}\) to 0 over the course of training, following common practice for MuJoCo environments (Code reference: \texttt{ppo2/ppo2.py\#L133-L135}).
    \item \textbf{Generalized Advantage Estimation (GAE):} GAE was employed for estimating advantage values.
    \item \textbf{Advantage Normalization:} Calculated advantages were normalized at the minibatch level by subtracting the mean and dividing by the standard deviation.
    \item \textbf{Global Gradient Clipping:} The gradients of the policy and value networks were rescaled during each update iteration to ensure the global L2 norm did not exceed 0.5. This technique has been shown to offer potential performance benefits \cite{andrychowicz2021what}.
    \item \textbf{Separate Networks:} Distinct neural networks were used for the policy (actor) and value (critic) functions. The architectures are outlined below:
    \begin{lstlisting}[language=Python]
value_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, 1), std=1.0),
)
policy_network = Sequential(
    layer_init(Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
    Tanh(),
    layer_init(Linear(64, 64)),
    Tanh(),
    layer_init(Linear(64, envs.single_action_space.n), std=0.01),
)
# Usage example:
# value = value_network(observation)
# action_dist = Categorical(policy_network(observation))
# action = action_dist.sample()
    \end{lstlisting}
\end{itemize}

\subsubsection{MuJoCo-Specific PPO Algorithm Details}
For environments involving continuous action spaces, specifically MuJoCo tasks, the following details were incorporated:
\begin{itemize}
    \item \textbf{Continuous Actions:} Actions were sampled from a Normal distribution. The policy network outputs the mean, while the standard deviation is handled separately.
    \item \textbf{State-Independent Log Standard Deviation:} The logarithm of the standard deviation for the action distribution was maintained as a state-independent parameter, initialized to zero.
    \item \textbf{Action Clipping and Storage:} Sampled continuous actions were clipped to the valid range defined by the environment. However, the original, unclipped action was stored in the experience buffer.
    \item \textbf{Reward Scaling:} Rewards were scaled by dividing them by the standard deviation of a rolling discounted sum of rewards. This technique, often implemented via wrappers like \texttt{VecNormalize}, is recommended for potentially improving performance \cite{Engstrom2020Implementation}.
\end{itemize}

\subsection{Environments}
We evaluated the PPO algorithm on three environments from the Gymnasium library \cite{towers2024gymnasium}, selected for their continuous action spaces: Pendulum-v1, InvertedPendulum-v4, and Ant-v4. The Pendulum and Inverted Pendulum environments represent classic control problems with relatively simple dynamics, while the Ant environment presents a more complex locomotion task. This selection allows for testing the algorithm's performance across varying levels of complexity within continuous control scenarios.

\subsection{Experiment Design}

\paragraph{Setup}
To structure the experiments, an abstract \texttt{Agent} class was defined, along with a specific implementation for the \texttt{PPOAgent}. The Gymnasium library facilitated environment creation and interaction. A \texttt{RunConfig} class was implemented to manage experiment configurations, ensuring parameters were consistently applied. Reproducibility was addressed by seeding the environment, agent initialization, and training process for each run.

\paragraph{Parameter Sweep and Replication}
To assess the impact of learning rates on performance, we conducted a grid search over different configurations for the actor and critic optimizers. The searched values were centered around those suggested by Huang et al. \cite{shengyi2022the37implementation}:
\begin{itemize}
    \item Actor Learning Rates: \(1 \times 10^{-3}\), \textbf{\(3 \times 10^{-4}\)} (baseline), \(1 \times 10^{-4}\)
    \item Critic Learning Rates: \(3 \times 10^{-3}\), \textbf{\(1 \times 10^{-3}\)} (baseline), \(3 \times 10^{-4}\)
\end{itemize}
This resulted in 9 distinct learning rate configurations per environment. For statistical robustness, each configuration was executed with 5 different random seeds.

\paragraph{Performance Evaluation}
Agent performance was measured by averaging the episodic reward over the final 20 episodes of training. This metric provides a stable estimate of the converged performance for each run. To determine if observed performance differences between learning rate configurations were statistically significant within each environment, independent two-sample t-tests were planned, and an alpha level of 0.05 was fixed a priori.

\paragraph{Total Runs}
The experimental setup resulted in a total of 45 runs per environment (3 actor rates \(\times\) 3 critic rates \(\times\) 5 seeds). Across the three environments, this amounted to 135 individual training runs.
\[
(3 \text{ actor learning rates} \times 3 \text{ critic learning rates} \times 5 \text{ seeds}) \times 3 \text{ environments} = 135 \text{ total runs}
\]

\section{Results}
\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/individual_rewards_curves_pendulum.png}
        \caption{Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/individual_rewards_curves_inverted_pendulum.png}
        \caption{Inverted Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/individual_rewards_curves_ant.png}
        \caption{Ant}
    \end{subfigure}
    \captionsetup{justification=centering} \
    \caption{Rewards curves for each environment. Each row represents a different seed, each column represents a different learning rate configuration}
    \label{fig:rewards_curves}
\end{figure}

The configurations for each environment were run long enough to converge. The reward curves, including the actual episode reward and the moving average reward, are shown in  \autoref{fig:rewards_curves}. The reward from the final 20 episodes of each run were averaged to and used as the measure of performance for that run. Further analysis of this performance follows.

\subsection{Rejecting the Null Hypothesis}
\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_pval_pendulum.png}
        \caption{Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_pval_inverted_pendulum.png}
        \caption{Inverted Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_pval_ant.png}
        \caption{Ant}
    \end{subfigure}
    \captionsetup{justification=centering} \
    \caption{Pairwise p-values for each environment. Pairs with a p-value less than $\alpha = 0.05$ are highlighted in yellow.}
    \label{fig:pairwise_comparison_pval}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
            \includegraphics[width=\textwidth]{figures/pairwise_comparison_diff_pendulum.png}
        \caption{Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_diff_inverted_pendulum.png}
        \caption{Inverted Pendulum}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pairwise_comparison_diff_ant.png}
        \caption{Ant}
    \end{subfigure}
    \captionsetup{justification=centering} \
    \caption{Pairwise differences for each environment. Pairs with a p-value less than $\alpha = 0.05$ are highlighted in yellow.}
    \label{fig:pairwise_comparison_diff}
\end{figure}


We first aim to reject the null hypothesis, that the mean reward of any two configurations are the same. We do this by performing a two-sample t-test for each environment, for each configuration, for each environment. Recall that we used 5 seeds for each configuration and an alpha level of 0.05. The results are shown in the \autoref{fig:pairwise_comparison_pval}. We immediately notice that certain configurations have a performance advantage which is statistically significant over others. To quantify the effect size of the performance difference, \autoref{fig:pairwise_comparison_diff} shows the difference in performance between the configurations.

\subsection{Identifying the best configurations}

\paragraph{Pendulum}
For the pendulum environment, we see that there is no signifcant difference caused by varying the critic learning rate (\autoref{fig:critic_lr_pval_pendulum}). However, there is a significant difference caused by varying the actor learning rate (\autoref{fig:actor_lr_pval_pendulum}). \autoref{fig:learning_rate_comparison_pendulum} shows the boxen plot of the performance of the different learning rate configurations. The best actor learning rate is \(3E-4\), which is the same as the baseline.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/learning_rate_comparison_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Boxen plot, comparing performance of learning rate configurations}
        \label{fig:learning_rate_comparison_pendulum}
    \end{subfigure}
    \\
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_pval_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Critic LR p-values}
        \label{fig:critic_lr_pval_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_diff_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Critic LR difference}
        \label{fig:critic_lr_diff_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_pval_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Actor LR p-values}
        \label{fig:actor_lr_pval_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_diff_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Actor LR difference}
        \label{fig:actor_lr_diff_pendulum}
    \end{subfigure}
    \caption{Pendulum learning rate comparison}
    \label{fig:pendulum_analysis}
\end{figure}

\paragraph{Inverted Pendulum}
For the Inverted Pendulum environment, we again see that there is no significant difference caused by varying the critic learning rate (\autoref{fig:critic_lr_pval_inverted_pendulum}), although we see some evidence that the \(1E-3\) critic learning rate is superior.

There is a significant difference caused by varying the actor learning rate (\autoref{fig:actor_lr_pval_inverted_pendulum}). \autoref{fig:learning_rate_comparison_inverted_pendulum} shows the boxen plot of the performance of the different learning rate configurations. The best actor learning rate is \(1E-3\), which differs from the baseline of \(3E-4\). We expect this higher learning rate to be better because the environment is so simple, and there is an advantage to be gained from a more aggressive learning rate. The effect size of the difference (\autoref{fig:actor_lr_diff_inverted_pendulum}) is not so large, but it is still significant.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/learning_rate_comparison_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Boxen plot, comparing performance of learning rate configurations}
        \label{fig:learning_rate_comparison_inverted_pendulum}
    \end{subfigure}
    \\
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_pval_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Critic LR p-values}
        \label{fig:critic_lr_pval_inverted_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_diff_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Critic LR difference}
        \label{fig:critic_lr_diff_inverted_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_pval_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Actor LR p-values}
        \label{fig:actor_lr_pval_inverted_pendulum}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_diff_inverted_pendulum.png}
        \captionsetup{justification=centering}
        \caption{Actor LR difference}
        \label{fig:actor_lr_diff_inverted_pendulum}
    \end{subfigure}
    \caption{Inverted Pendulum learning rate comparison}
    \label{fig:inverted_pendulum_analysis}
\end{figure}

\paragraph{Ant}
For the Ant environment, we again see that there is no significant difference caused by varying the critic learning rate (\autoref{fig:critic_lr_pval_ant}). However, there is a significant difference caused by varying the actor learning rate (\autoref{fig:actor_lr_pval_ant}). The effect size of the difference (\autoref{fig:actor_lr_diff_ant}) is much larger than that of the other two environments, indicating the importance of tuning the actor learning rate for this complex environment.

\autoref{fig:learning_rate_comparison_ant} shows the boxen plot of the performance of the different learning rate configurations. The best actor learning rate is \(3E-4\), which is the same as the baseline.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/learning_rate_comparison_ant.png}
        \captionsetup{justification=centering}
        \caption{Boxen plot, comparing performance of learning rate configurations}
        \label{fig:learning_rate_comparison_ant}
    \end{subfigure}
    \\
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_pval_ant.png}
        \captionsetup{justification=centering}
        \caption{Critic LR p-values}
        \label{fig:critic_lr_pval_ant}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/critic_lr_diff_ant.png}
        \captionsetup{justification=centering}
        \caption{Critic LR difference}
        \label{fig:critic_lr_diff_ant}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_pval_ant.png}
        \captionsetup{justification=centering}
        \caption{Actor LR p-values}
        \label{fig:actor_lr_pval_ant}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/actor_lr_diff_ant.png}
        \captionsetup{justification=centering}
        \caption{Actor LR difference}
        \label{fig:actor_lr_diff_ant}
    \end{subfigure}
    \caption{Ant learning rate comparison}
    \label{fig:ant_analysis}
\end{figure}

\section{Conclusion}
Summarize the main findings of the report. Discuss the implications of your results, acknowledge any limitations, and suggest potential future work.

\bibliographystyle{plain} % Style for the bibliography (e.g., plain, unsrt, alpha)
\bibliography{references} % Name of your .bib file (without the .bib extension)

\end{document}
